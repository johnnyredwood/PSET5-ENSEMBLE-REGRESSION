{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e29b0efe-78c5-45f3-97cc-9271ed2de92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n",
      "Collecting snowflake-connector-python\n",
      "  Downloading snowflake_connector_python-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m773.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (4.8.0)\n",
      "Collecting filelock<4,>=3.5 (from snowflake-connector-python)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.11.0)\n",
      "Collecting tomlkit (from snowflake-connector-python)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting boto3>=1.24 (from snowflake-connector-python)\n",
      "  Downloading boto3-1.40.75-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting botocore>=1.24 (from snowflake-connector-python)\n",
      "  Downloading botocore-1.40.75-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.24->snowflake-connector-python)\n",
      "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=3.1.0->snowflake-connector-python) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.1.0->snowflake-connector-python) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.16.0)\n",
      "Downloading snowflake_connector_python-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.40.75-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m933.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.40.75-py3-none-any.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m778.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: asn1crypto, tomlkit, jmespath, filelock, botocore, s3transfer, boto3, snowflake-connector-python\n",
      "Successfully installed asn1crypto-1.5.1 boto3-1.40.75 botocore-1.40.75 filelock-3.20.0 jmespath-1.0.1 s3transfer-0.14.0 snowflake-connector-python-4.0.0 tomlkit-0.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acee23d5-1b8a-4216-bcac-c7e3a520ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d549fe34-75ea-49af-b7d5-057d00224ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-03 01:31:06--  https://jdbc.postgresql.org/download/postgresql-42.2.5.jar\n",
      "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
      "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 825943 (807K) [application/java-archive]\n",
      "Saving to: ‘./postgresql-42.2.5.jar.3’\n",
      "\n",
      "postgresql-42.2.5.j 100%[===================>] 806.58K  1.08MB/s    in 0.7s    \n",
      "\n",
      "2025-11-03 01:31:07 (1.08 MB/s) - ‘./postgresql-42.2.5.jar.3’ saved [825943/825943]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.5.jar -P ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac03bfc-da81-4664-96cb-2dea96a5db9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAR file exists: True\n",
      "JAR file size: 825943 bytes\n",
      "PORT_POSTGRES: 5432\n",
      "POSTGRES_DB: ny_taxi\n",
      "POSTGRES_USER: usuario_spark\n",
      "POSTGRES_PASSWORD set: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "jar_path = \"/home/jovyan/work/postgresql-42.2.5.jar\"\n",
    "print(f\"JAR file exists: {os.path.exists(jar_path)}\")\n",
    "print(f\"JAR file size: {os.path.getsize(jar_path) if os.path.exists(jar_path) else 'N/A'} bytes\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"PORT_POSTGRES: {os.getenv('PORT_POSTGRES')}\")\n",
    "print(f\"POSTGRES_DB: {os.getenv('POSTGRES_DB')}\")\n",
    "print(f\"POSTGRES_USER: {os.getenv('POSTGRES_USER')}\")\n",
    "print(f\"POSTGRES_PASSWORD set: {bool(os.getenv('POSTGRES_PASSWORD'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51ec312-09a1-4bad-8490-84f7f6e77c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import Row \n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/home/jovyan/work/postgresql-42.2.5.jar\").master(\"local\").appName(\"PySpark_Postgres_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04f87aa-eec4-4537-89ed-675318364229",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"raw.taxi_zone_lookup\") \\\n",
    "    .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "    .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa1936c-4fe2-4004-8b80-e60c9f844ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- locationid: integer (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      " |-- ingested_at_utc: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca0aa0f-be73-492c-a04a-0a216bcf328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingestar_zones_a_raw():\n",
    "    SOURCE_PATH = os.getenv(\"SOURCE_PATH\")\n",
    "    path_url = f\"{SOURCE_PATH}/misc/taxi_zone_lookup.csv\"\n",
    "    local_path = f\"/tmp/taxiZones.parquet\"\n",
    "    \n",
    "    # Descargo el archivo en carpeta temporal para posteriormente leerlo\n",
    "    try:\n",
    "        r = requests.get(path_url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=10000000):\n",
    "                    f.write(chunk)\n",
    "        else:\n",
    "            print(f\"Archivo no encontrado en {path_url} (status {r.status_code})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {path_url}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo obtenido exitosamente de: {path_url}\")\n",
    "    \n",
    "    # Leo el archivo en un df de Spark\n",
    "    try:\n",
    "        df = spark.read.csv(local_path, header=\"true\")\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo leer {local_path}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo leido exitosamente por Spark: {local_path}\")\n",
    "\n",
    "    conteoFilas = df.count()\n",
    "    print(f\"Ingestando hacia Snowflake datos de Zonas de Taxis. Total de filas: {conteoFilas}\")\n",
    "\n",
    "    try:\n",
    "        df.write.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"dbtable\", \"raw.taxi_zone_lookup\") \\\n",
    "            .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "            .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "    except Exception as e2:\n",
    "        print(f\"Error con ingreso de datos: {e2}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"Zonas de taxis exportadas correctamente a Raw de Snowflake\")\n",
    "\n",
    "    #Me aseguro de eliminar el archivo parquet temporal\n",
    "    try:\n",
    "        os.remove(local_path)\n",
    "        print(f\"Archivo parquet temporal removido: {local_path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"No se pudo remover el archivo parquet temporal {local_path}: {e}\")\n",
    "\n",
    "    #Retorno datos para tabla de conteos de datos consumidos por run\n",
    "    return {\n",
    "        \"count\": conteoFilas,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5baf7d12-66d1-4ca2-95f5-3c1b60011838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Archivo leido exitosamente por Spark: /tmp/taxiZones.parquet\n",
      "Ingestando hacia Snowflake datos de Zonas de Taxis. Total de filas: 265\n",
      "Zonas de taxis exportadas correctamente a Raw de Snowflake\n",
      "Archivo parquet temporal removido: /tmp/taxiZones.parquet\n",
      "{'count': 265}\n"
     ]
    }
   ],
   "source": [
    "#Cargo Zonas de Taxis en Snowflake\n",
    "zonasTaxisIngesta=ingestar_zones_a_raw()\n",
    "print(zonasTaxisIngesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4490de1-45aa-48b9-8744-67fb80c27515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#Hago la presente funcion para generar un identificador unico asociado a cada carga de datos para el RUN_ID \n",
    "def generar_run_id():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1267211c-a36a-46b8-95fd-c3d84e0911d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests \n",
    "from pyspark.sql.functions import lit, current_timestamp \n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def ingestar_parquet_a_raw(service: str, year: int, month: int):\n",
    "    SOURCE_PATH = os.getenv(\"SOURCE_PATH\")\n",
    "    path_url = f\"{SOURCE_PATH}/trip-data/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    local_path = f\"/tmp/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    print(path_url)\n",
    "    \n",
    "    # Descargo el archivo Parquet en carpeta temporal para posteriormente leerlo\n",
    "    try:\n",
    "        r = requests.get(path_url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=10000000):\n",
    "                    f.write(chunk)\n",
    "        else:\n",
    "            print(f\"Archivo no encontrado en {path_url} (status {r.status_code})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {path_url}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo obtenido exitosamente de: {path_url}\")\n",
    "    \n",
    "    # Leo el archivo parquet en un df de Spark\n",
    "    try:\n",
    "        df = spark.read.parquet(local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo leer {local_path}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo leido exitosamente por Spark: {local_path}\")\n",
    "\n",
    "    run_id = generar_run_id()\n",
    "\n",
    "    # Elimino columna conflictiva que solo esta presente en unos pocos parquets\n",
    "    if 'cbd_congestion_fee' in df.columns:\n",
    "        df = df.drop('cbd_congestion_fee')\n",
    "\n",
    "    # Añado los metadatos indicados en instrucciones de PSET\n",
    "    df_meta = df.withColumn(\"run_id\", lit(run_id)) \\\n",
    "                .withColumn(\"service_type\", lit(service)) \\\n",
    "                .withColumn(\"source_year\", lit(year)) \\\n",
    "                .withColumn(\"source_month\", lit(month)) \\\n",
    "                .withColumn(\"ingested_at_utc\", current_timestamp()) \\\n",
    "                .withColumn(\"source_path\", lit(path_url))\n",
    "\n",
    "    # Convierto tipos de fecha a Timestamp porque me estaba marcando error al enviar datos al Snowflake sin esta transformacion\n",
    "    for field in df_meta.schema.fields:\n",
    "        if field.dataType.typeName() == \"timestamp_ntz\":\n",
    "            df_meta = df_meta.withColumn(field.name, df_meta[field.name].cast(TimestampType()))\n",
    "\n",
    "    primary_keys = [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"PULocationID\",\"DoLocationID\",\"VendorID\"]\n",
    "\n",
    "    if (service==\"green\"):\n",
    "        primary_keys = [\"lpep_pickup_datetime\",\"lpep_dropoff_datetime\",\"PULocationID\",\"DoLocationID\",\"VendorID\"]\n",
    "\n",
    "    df_meta = df_meta.dropDuplicates(primary_keys)\n",
    "\n",
    "    conteoFilas = df_meta.count()\n",
    "    print(f\"Ingestando hacia Snowflake {service} {year}-{month}. Total de filas: {conteoFilas}\")\n",
    "\n",
    "    try:\n",
    "        df_meta.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", f\"raw.{service}_taxi_trip\") \\\n",
    "        .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "        .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    except Exception as e2:\n",
    "        print(f\"Error con tabla temporal: {e2}\")\n",
    "        return None\n",
    "\n",
    "    #Me aseguro de eliminar el archivo parquet temporal\n",
    "    try:\n",
    "        os.remove(local_path)\n",
    "        print(f\"Archivo parquet temporal removido: {local_path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"No se pudo remover el archivo parquet temporal {local_path}: {e}\")\n",
    "\n",
    "    #Retorno datos para tabla de conteos de datos consumidos por run\n",
    "    return {\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"count\": conteoFilas,\n",
    "        \"run_id\": run_id,\n",
    "        \"service_type\": service\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8bdfd-f83d-41a8-9111-1f457512bdff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointTaxisYellow.json\n",
      "checkpoint: {'year': 0, 'month': 0}\n",
      "Iniciando ingesta de datos de taxis yellow: 1-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/yellow_tripdata_2022-01.parquet\n",
      "Ingestando hacia Snowflake yellow 2022-1. Total de filas: 2452562\n",
      "Archivo parquet temporal removido: /tmp/yellow_tripdata_2022-01.parquet\n",
      "Iniciando ingesta de datos de taxis yellow: 2-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/yellow_tripdata_2022-02.parquet\n",
      "Ingestando hacia Snowflake yellow 2022-2. Total de filas: 2965646\n",
      "Archivo parquet temporal removido: /tmp/yellow_tripdata_2022-02.parquet\n",
      "Iniciando ingesta de datos de taxis yellow: 3-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/yellow_tripdata_2022-03.parquet\n",
      "Ingestando hacia Snowflake yellow 2022-3. Total de filas: 3610377\n",
      "Archivo parquet temporal removido: /tmp/yellow_tripdata_2022-03.parquet\n",
      "Iniciando ingesta de datos de taxis yellow: 4-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/yellow_tripdata_2022-04.parquet\n",
      "Ingestando hacia Snowflake yellow 2022-4. Total de filas: 3582775\n",
      "Archivo parquet temporal removido: /tmp/yellow_tripdata_2022-04.parquet\n",
      "Iniciando ingesta de datos de taxis yellow: 5-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/yellow_tripdata_2022-05.parquet\n",
      "Ingestando hacia Snowflake yellow 2022-5. Total de filas: 3569995\n",
      "Archivo parquet temporal removido: /tmp/yellow_tripdata_2022-05.parquet\n",
      "Iniciando ingesta de datos de taxis yellow: 6-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/yellow_tripdata_2022-06.parquet\n",
      "Ingestando hacia Snowflake yellow 2022-6. Total de filas: 3537942\n",
      "Archivo parquet temporal removido: /tmp/yellow_tripdata_2022-06.parquet\n",
      "Iniciando ingesta de datos de taxis yellow: 7-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/yellow_tripdata_2022-07.parquet\n",
      "Ingestando hacia Snowflake yellow 2022-7. Total de filas: 3155488\n",
      "Archivo parquet temporal removido: /tmp/yellow_tripdata_2022-07.parquet\n",
      "Iniciando ingesta de datos de taxis yellow: 8-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/yellow_tripdata_2022-08.parquet\n",
      "Ingestando hacia Snowflake yellow 2022-8. Total de filas: 3132963\n",
      "Archivo parquet temporal removido: /tmp/yellow_tripdata_2022-08.parquet\n",
      "Iniciando ingesta de datos de taxis yellow: 9-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/yellow_tripdata_2022-09.parquet\n",
      "Ingestando hacia Snowflake yellow 2022-9. Total de filas: 3164832\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "#Defino funciones tipicas de checkpoint para en caso de fallo no ingestar datos desde cero\n",
    "def save_checkpoint(year, month):\n",
    "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump({\"year\": year, \"month\": month}, f)\n",
    "\n",
    "def load_checkpoint(CHECKPOINT_FILE):\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"year\": 0, \"month\": 0}\n",
    "\n",
    "#Genero arreglo que contendra datos de ingesta para tabla de conteos\n",
    "resultadosGeneralesIngesta=[]\n",
    "\n",
    "try:\n",
    "    #Leo los datos de tipos de taxis, years y months desde variables de entorno\n",
    "    tipos_taxis=os.getenv(\"SERVICES\").split(',')\n",
    "    for tipo_taxi in tipos_taxis:\n",
    "        \n",
    "        lista_years = sorted([int(item) for item in (os.getenv(\"YEARS\").split(','))])\n",
    "        lista_months = sorted([int(item) for item in (os.getenv(\"MONTHS\").split(','))])\n",
    "        #Cargo el checkpoint y en caso de que tenga registros recorto los arreglos de months y years para recorrer desde ultima ingesta exitosa\n",
    "        CHECKPOINT_FILE = f\"checkpointTaxis{tipo_taxi.capitalize()}.json\"\n",
    "        print(CHECKPOINT_FILE)\n",
    "        checkpoint=load_checkpoint(CHECKPOINT_FILE)\n",
    "        print(f\"checkpoint: {checkpoint}\")\n",
    "        \n",
    "        if ( checkpoint != {\"year\": 0, \"month\": 0} and (int(checkpoint[\"month\"]) in lista_months) and (int(checkpoint[\"year\"]) in lista_years)):    \n",
    "            if ( int(checkpoint[\"month\"]) == lista_months[-1] and int(checkpoint[\"year\"]) != lista_years[-1] ):\n",
    "                lista_years= lista_years[lista_years.index(checkpoint[\"year\"])+1:]\n",
    "            elif ( int(checkpoint[\"month\"]) == lista_months[-1] and int(checkpoint[\"year\"]) == lista_years[-1] ): \n",
    "                continue\n",
    "            else:\n",
    "                lista_years= lista_years[lista_years.index(checkpoint[\"year\"]):]\n",
    "                lista_months= lista_months[lista_months.index(checkpoint[\"month\"])+1:]\n",
    "        \n",
    "        for year_taxi in lista_years:\n",
    "            for month_taxi in lista_months:  \n",
    "                #Llamo a la funcion de ingesta de datos iterativamente para cada mes, year y tipo de taxi\n",
    "                print(f\"Iniciando ingesta de datos de taxis {tipo_taxi}: {month_taxi}-{year_taxi}\")\n",
    "                resultadosParciales=ingestar_parquet_a_raw(tipo_taxi, year_taxi, month_taxi)\n",
    "                #Guardo los resultados y genero el checkpint\n",
    "                if (resultadosParciales != None):\n",
    "                    resultadosGeneralesIngesta.append(resultadosParciales)\n",
    "                    save_checkpoint(year_taxi,month_taxi)\n",
    "            lista_months = sorted([int(item) for item in (os.getenv(\"MONTHS\").split(','))])\n",
    "                \n",
    "except Exception as e5:\n",
    "    #Como en todas las funciones vistas hago manejo de errores\n",
    "    print(f\"Fallo el proceso de ingesta masiva de datos de taxis NY: {e5}\")\n",
    "else:\n",
    "    print(\"El proceso de ingesta masiva de taxis NY fue exitoso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c170d37-70c1-481f-b4f0-349a99123c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
