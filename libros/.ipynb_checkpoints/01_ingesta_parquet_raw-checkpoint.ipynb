{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b0efe-78c5-45f3-97cc-9271ed2de92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n",
      "Collecting snowflake-connector-python\n",
      "  Downloading snowflake_connector_python-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (4.8.0)\n",
      "Collecting filelock<4,>=3.5 (from snowflake-connector-python)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.11.0)\n",
      "Collecting tomlkit (from snowflake-connector-python)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting boto3>=1.24 (from snowflake-connector-python)\n",
      "  Downloading boto3-1.40.64-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore>=1.24 (from snowflake-connector-python)\n",
      "  Downloading botocore-1.40.64-py3-none-any.whl.metadata (5.7 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee23d5-1b8a-4216-bcac-c7e3a520ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549fe34-75ea-49af-b7d5-057d00224ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.5.jar -P ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac03bfc-da81-4664-96cb-2dea96a5db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "jar_path = \"/home/jovyan/work/postgresql-42.2.5.jar\"\n",
    "print(f\"JAR file exists: {os.path.exists(jar_path)}\")\n",
    "print(f\"JAR file size: {os.path.getsize(jar_path) if os.path.exists(jar_path) else 'N/A'} bytes\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"PORT_POSTGRES: {os.getenv('PORT_POSTGRES')}\")\n",
    "print(f\"POSTGRES_DB: {os.getenv('POSTGRES_DB')}\")\n",
    "print(f\"POSTGRES_USER: {os.getenv('POSTGRES_USER')}\")\n",
    "print(f\"POSTGRES_PASSWORD set: {bool(os.getenv('POSTGRES_PASSWORD'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ec312-09a1-4bad-8490-84f7f6e77c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import Row \n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/home/jovyan/work/postgresql-42.2.5.jar\").master(\"local\").appName(\"PySpark_Postgres_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f87aa-eec4-4537-89ed-675318364229",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"raw.taxi_zone_lookup\") \\\n",
    "    .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "    .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1936c-4fe2-4004-8b80-e60c9f844ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0aa0f-be73-492c-a04a-0a216bcf328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingestar_zones_a_raw():\n",
    "    SOURCE_PATH = os.getenv(\"SOURCE_PATH\")\n",
    "    path_url = f\"{SOURCE_PATH}/misc/taxi_zone_lookup.csv\"\n",
    "    local_path = f\"/tmp/taxiZones.parquet\"\n",
    "    \n",
    "    # Descargo el archivo en carpeta temporal para posteriormente leerlo\n",
    "    try:\n",
    "        r = requests.get(path_url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=10000000):\n",
    "                    f.write(chunk)\n",
    "        else:\n",
    "            print(f\"Archivo no encontrado en {path_url} (status {r.status_code})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {path_url}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo obtenido exitosamente de: {path_url}\")\n",
    "    \n",
    "    # Leo el archivo en un df de Spark\n",
    "    try:\n",
    "        df = spark.read.csv(local_path, header=\"true\")\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo leer {local_path}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo leido exitosamente por Spark: {local_path}\")\n",
    "\n",
    "    conteoFilas = df.count()\n",
    "    print(f\"Ingestando hacia Snowflake datos de Zonas de Taxis. Total de filas: {conteoFilas}\")\n",
    "\n",
    "    try:\n",
    "        df.write.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"dbtable\", \"raw.taxi_zone_lookup\") \\\n",
    "            .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "            .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "    except Exception as e2:\n",
    "        print(f\"Error con ingreso de datos: {e2}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"Zonas de taxis exportadas correctamente a Raw de Snowflake\")\n",
    "\n",
    "    #Me aseguro de eliminar el archivo parquet temporal\n",
    "    try:\n",
    "        os.remove(local_path)\n",
    "        print(f\"Archivo parquet temporal removido: {local_path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"No se pudo remover el archivo parquet temporal {local_path}: {e}\")\n",
    "\n",
    "    #Retorno datos para tabla de conteos de datos consumidos por run\n",
    "    return {\n",
    "        \"count\": conteoFilas,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf7d12-66d1-4ca2-95f5-3c1b60011838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargo Zonas de Taxis en Snowflake\n",
    "zonasTaxisIngesta=ingestar_zones_a_raw()\n",
    "print(zonasTaxisIngesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4490de1-45aa-48b9-8744-67fb80c27515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#Hago la presente funcion para generar un identificador unico asociado a cada carga de datos para el RUN_ID \n",
    "def generar_run_id():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267211c-a36a-46b8-95fd-c3d84e0911d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests \n",
    "from pyspark.sql.functions import lit, current_timestamp \n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def ingestar_parquet_a_raw(service: str, year: int, month: int):\n",
    "    SOURCE_PATH = os.getenv(\"SOURCE_PATH\")\n",
    "    path_url = f\"{SOURCE_PATH}/trip-data/yellow_tripdata_{year}-{month:02d}.parquet\"\n",
    "    local_path = f\"/tmp/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    \n",
    "    # Descargo el archivo Parquet en carpeta temporal para posteriormente leerlo\n",
    "    try:\n",
    "        r = requests.get(path_url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=10000000):\n",
    "                    f.write(chunk)\n",
    "        else:\n",
    "            print(f\"Archivo no encontrado en {path_url} (status {r.status_code})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {path_url}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo obtenido exitosamente de: {path_url}\")\n",
    "    \n",
    "    # Leo el archivo parquet en un df de Spark\n",
    "    try:\n",
    "        df = spark.read.parquet(local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo leer {local_path}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo leido exitosamente por Spark: {local_path}\")\n",
    "\n",
    "    run_id = generar_run_id()\n",
    "\n",
    "    # Elimino columna conflictiva que solo esta presente en unos pocos parquets\n",
    "    if 'cbd_congestion_fee' in df.columns:\n",
    "        df = df.drop('cbd_congestion_fee')\n",
    "\n",
    "    # Añado los metadatos indicados en instrucciones de PSET\n",
    "    df_meta = df.withColumn(\"run_id\", lit(run_id)) \\\n",
    "                .withColumn(\"service_type\", lit(service)) \\\n",
    "                .withColumn(\"source_year\", lit(year)) \\\n",
    "                .withColumn(\"source_month\", lit(month)) \\\n",
    "                .withColumn(\"ingested_at_utc\", current_timestamp()) \\\n",
    "                .withColumn(\"source_path\", lit(path_url))\n",
    "\n",
    "    # Convierto tipos de fecha a Timestamp porque me estaba marcando error al enviar datos al Snowflake sin esta transformacion\n",
    "    for field in df_meta.schema.fields:\n",
    "        if field.dataType.typeName() == \"timestamp_ntz\":\n",
    "            df_meta = df_meta.withColumn(field.name, df_meta[field.name].cast(TimestampType()))\n",
    "\n",
    "    primary_keys = [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"PULocationID\",\"DoLocationID\",\"VendorID\"]\n",
    "\n",
    "    if (service==\"green\"):\n",
    "        primary_keys = [\"lpep_pickup_datetime\",\"lpep_dropoff_datetime\",\"PULocationID\",\"DoLocationID\",\"VendorID\"]\n",
    "\n",
    "    df_meta = df_meta.dropDuplicates(primary_keys)\n",
    "\n",
    "    conteoFilas = df_meta.count()\n",
    "    print(f\"Ingestando hacia Snowflake {service} {year}-{month}. Total de filas: {conteoFilas}\")\n",
    "\n",
    "    try:\n",
    "        df_meta.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", f\"raw.{service}_taxi_trip\") \\\n",
    "        .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "        .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    except Exception as e2:\n",
    "        print(f\"Error con tabla temporal: {e2}\")\n",
    "        return None\n",
    "\n",
    "    #Me aseguro de eliminar el archivo parquet temporal\n",
    "    try:\n",
    "        os.remove(local_path)\n",
    "        print(f\"Archivo parquet temporal removido: {local_path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"No se pudo remover el archivo parquet temporal {local_path}: {e}\")\n",
    "\n",
    "    #Retorno datos para tabla de conteos de datos consumidos por run\n",
    "    return {\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"count\": conteoFilas,\n",
    "        \"run_id\": run_id,\n",
    "        \"service_type\": service\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8bdfd-f83d-41a8-9111-1f457512bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "#Defino funciones tipicas de checkpoint para en caso de fallo no ingestar datos desde cero\n",
    "def save_checkpoint(year, month):\n",
    "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump({\"year\": year, \"month\": month}, f)\n",
    "\n",
    "def load_checkpoint(CHECKPOINT_FILE):\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"year\": 0, \"month\": 0}\n",
    "\n",
    "#Genero arreglo que contendra datos de ingesta para tabla de conteos\n",
    "resultadosGeneralesIngesta=[]\n",
    "\n",
    "try:\n",
    "    #Leo los datos de tipos de taxis, years y months desde variables de entorno\n",
    "    tipos_taxis=os.getenv(\"SERVICES\").split(',')\n",
    "    for tipo_taxi in tipos_taxis:\n",
    "        \n",
    "        lista_years = sorted([int(item) for item in (os.getenv(\"YEARS\").split(','))])\n",
    "        lista_months = sorted([int(item) for item in (os.getenv(\"MONTHS\").split(','))])\n",
    "        #Cargo el checkpoint y en caso de que tenga registros recorto los arreglos de months y years para recorrer desde ultima ingesta exitosa\n",
    "        CHECKPOINT_FILE = f\"checkpointTaxis{tipo_taxi.capitalize()}.json\"\n",
    "        print(CHECKPOINT_FILE)\n",
    "        checkpoint=load_checkpoint(CHECKPOINT_FILE)\n",
    "        print(f\"checkpoint: {checkpoint}\")\n",
    "        \n",
    "        if ( checkpoint != {\"year\": 0, \"month\": 0} and (int(checkpoint[\"month\"]) in lista_months) and (int(checkpoint[\"year\"]) in lista_years)):    \n",
    "            if ( int(checkpoint[\"month\"]) == lista_months[-1] and int(checkpoint[\"year\"]) != lista_years[-1] ):\n",
    "                lista_years= lista_years[lista_years.index(checkpoint[\"year\"])+1:]\n",
    "            elif ( int(checkpoint[\"month\"]) == lista_months[-1] and int(checkpoint[\"year\"]) == lista_years[-1] ): \n",
    "                continue\n",
    "            else:\n",
    "                lista_years= lista_years[lista_years.index(checkpoint[\"year\"]):]\n",
    "                lista_months= lista_months[lista_months.index(checkpoint[\"month\"])+1:]\n",
    "        \n",
    "        for year_taxi in lista_years:\n",
    "            for month_taxi in lista_months:  \n",
    "                #Llamo a la funcion de ingesta de datos iterativamente para cada mes, year y tipo de taxi\n",
    "                print(f\"Iniciando ingesta de datos de taxis {tipo_taxi}: {month_taxi}-{year_taxi}\")\n",
    "                resultadosParciales=ingestar_parquet_a_raw(tipo_taxi, year_taxi, month_taxi)\n",
    "                #Guardo los resultados y genero el checkpint\n",
    "                if (resultadosParciales != None):\n",
    "                    resultadosGeneralesIngesta.append(resultadosParciales)\n",
    "                    save_checkpoint(year_taxi,month_taxi)\n",
    "            lista_months = sorted([int(item) for item in (os.getenv(\"MONTHS\").split(','))])\n",
    "                \n",
    "except Exception as e5:\n",
    "    #Como en todas las funciones vistas hago manejo de errores\n",
    "    print(f\"Fallo el proceso de ingesta masiva de datos de taxis NY: {e5}\")\n",
    "else:\n",
    "    print(\"El proceso de ingesta masiva de taxis NY fue exitoso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85008359-db5e-4e54-968f-e69ec7e000b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
